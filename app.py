# app.py
from __future__ import annotations

import json
import re
from collections import Counter
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import requests
import streamlit as st
from openai import OpenAI


# =========================
# ê¸°ë³¸ ì„¤ì •
# =========================
st.set_page_config(page_title="ì–¼êµ´í˜• ê¸°ë°˜ ë¯¸ìš©ì‹¤ ì¶”ì²œ", layout="wide")
st.title("ì–¼êµ´í˜• ê¸°ë°˜ ë¯¸ìš©ì‹¤ ì¶”ì²œ (Kakao Local + ì›¹í›„ê¸° ê¸°ë°˜ í™•ì¥ê²€ìƒ‰)")
st.caption("ìê°€ì§„ë‹¨ ì„ íƒ â†’ (GPT ì¶”ì²œ 3ê°œ) â†’ (ì›¹í›„ê¸° íƒœê·¸ ê¸°ë°˜ í™•ì¥ê²€ìƒ‰) â†’ ê·¼ì²˜ ë¯¸ìš©ì‹¤ ì¶”ì²œ")


# =========================
# âœ… ì‹¤ì¡´ í—¤ì–´ìŠ¤íƒ€ì¼/ì‹œìˆ  ìš©ì–´ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸
# =========================
STYLE_TERMS = [
    "ë‹¨ë°œ", "ì¤‘ë‹¨ë°œ", "ì¥ë°œ", "ìˆì»·", "ë³´ë¸Œì»·", "í—ˆì‰¬ì»·", "ë ˆì´ì–´ë“œì»·", "ìƒ¤ê¸°ì»·",
    "ë¦¬í”„ì»·", "ê°€ì¼ì»·", "íˆ¬ë¸”ëŸ­", "ëŒ„ë””ì»·", "í¬ë¡­ì»·",
    "Cì»¬íŒ", "Sì»¬íŒ", "ë¹Œë“œíŒ", "íˆí”¼íŒ", "ì‰ë„ìš°íŒ", "ê°€ë¥´ë§ˆíŒ", "ì• ì¦ˆíŒ",
    "ë¦¬ì  íŠ¸íŒ", "ì•„ì´ë¡±íŒ", "ë³¼ë¥¨íŒ", "ë””ì§€í„¸íŒ", "ì…‹íŒ…íŒ",
    "ë³¼ë¥¨ë§¤ì§", "ë§¤ì§", "ë§¤ì§ì…‹íŒ…",
    "ì—¼ìƒ‰", "íƒˆìƒ‰", "ë¿Œë¦¬ì—¼ìƒ‰", "ì˜´ë¸Œë ˆ", "ë°œë ˆì•„ì¥¬",
    "ì• ì‰¬ë¸Œë¼ìš´", "ì• ì‰¬ê·¸ë ˆì´", "ì• ì‰¬ë¸”ë£¨",
    "í•‘í¬ë¸Œë¼ìš´", "ì´ˆì½”ë¸Œë¼ìš´", "ì¹´í‚¤ë¸Œë¼ìš´",
    "ë‹¤ìš´íŒ", "ë‘í”¼ì¼€ì–´", "í´ë¦¬ë‹‰",
    "ì‹œìŠ¤ë£¨ë±…", "ì²˜í”¼ë±…", "í’€ë±…", "ì• êµë¨¸ë¦¬",
]
STYLE_STOP = {"ë¯¸ìš©ì‹¤", "í—¤ì–´", "ì»·", "íŒ", "ì—¼ìƒ‰"}


# =========================
# í•„ìš”í•œ ì´ë¯¸ì§€ íŒŒì¼ ì²´í¬
# =========================
REQUIRED_IMAGES = [
    "ì›œí†¤.jpg", "ì¿¨í†¤.jpg",
    "ê³„ë€í˜•.png", "ë§ˆë¦„ëª¨í˜•.png", "í•˜íŠ¸í˜•.png", "ë•…ì½©í˜•.png", "ìœ¡ê°í˜•.png", "ë‘¥ê·¼í˜•.png",
    "ì•„ì¹˜í˜•.png", "ì§ì„ í˜•.png", "ê°ì§„í˜•.png", "ë‘¥ê·¼í˜•(ëˆˆì¹).png",
    "ì§ëª¨.png", "ê³±ìŠ¬.png",
]


def must_exist(path: str) -> None:
    if not Path(path).exists():
        st.error(
            f"ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”: {path}\n\n"
            f"app.pyì™€ ê°™ì€ í´ë”ì— '{path}' íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
        )
        st.stop()


for p in REQUIRED_IMAGES:
    must_exist(p)


# =========================
# Kakao API í‚¤ ë¡œë“œ
# =========================
KAKAO_REST_API_KEY = (st.secrets.get("KAKAO_REST_API_KEY", "") or "").strip()
if not KAKAO_REST_API_KEY:
    st.warning("KAKAO_REST_API_KEYê°€ ì—†ì–´ìš”. .streamlit/secrets.tomlì— ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()


# =========================
# OpenAI API Key (ì‚¬ì´ë“œë°” ì…ë ¥)
# =========================
st.sidebar.header("ğŸ”‘ API Key ì„¤ì •")
OPENAI_API_KEY = st.sidebar.text_input("OpenAI API Key (í•„ìˆ˜)", type="password").strip()
st.sidebar.caption("ëª¨ë“  ì„ íƒ ì™„ë£Œ ì‹œ GPT ì¶”ì²œ(3ê°œ) + ì›¹í›„ê¸° ê¸°ë°˜ í™•ì¥ê²€ìƒ‰ìœ¼ë¡œ ë¯¸ìš©ì‹¤ì„ ì°¾ìŠµë‹ˆë‹¤.")


# =========================
# UI ì¹´ë“œ ë Œë” ìœ í‹¸ (âœ… 1í´ë¦­ ì¦‰ì‹œ ë°˜ì˜)
# =========================
def select_card(
    *,
    title: str,
    image_path: str,
    button_label: str,
    on_click_value: str,
    session_key: str,
    button_key: str,
    desc_md: str | None = None,
    img_width: int = 160,
    selected: bool = False,
) -> None:
    st.subheader(title)
    st.image(image_path, width=img_width)
    if desc_md:
        st.markdown(desc_md)

    btn_type = "primary" if selected else "secondary"
    if st.button(button_label, key=button_key, use_container_width=True, type=btn_type):
        st.session_state[session_key] = on_click_value
        st.rerun()


# =========================
# ì–¼êµ´í˜• â†’ íŒíŠ¸ ìš©ì–´
# =========================
FACE_SHAPE_TO_KEYWORDS: Dict[str, List[str]] = {
    "ë‘¥ê·¼ì–¼êµ´í˜•": ["ë ˆì´ì–´ë“œì»·", "Sì»¬íŒ", "Cì»¬íŒ", "ì‹œìŠ¤ë£¨ë±…"],
    "ê¸´ì–¼êµ´í˜•": ["ë‹¨ë°œ", "ì¤‘ë‹¨ë°œ", "Cì»¬íŒ", "íˆí”¼íŒ"],
    "ê°ì§„ ì–¼êµ´í˜•": ["ë ˆì´ì–´ë“œì»·", "Sì»¬íŒ", "ë³¼ë¥¨íŒ"],
    "ì—­ì‚¼ê°í˜• ì–¼êµ´": ["ë‹¨ë°œ", "Cì»¬íŒ", "ë³¼ë¥¨ë§¤ì§"],
    "ê³„ë€í˜• ì–¼êµ´": ["ë‹¨ë°œ", "ì¤‘ë‹¨ë°œ", "ë ˆì´ì–´ë“œì»·", "Sì»¬íŒ"],
}

APP_FACE_TO_RECO_FACE: Dict[str, str] = {
    "ë‘¥ê·¼í˜•": "ë‘¥ê·¼ì–¼êµ´í˜•",
    "ê³„ë€í˜•": "ê³„ë€í˜• ì–¼êµ´",
    "í•˜íŠ¸í˜•": "ì—­ì‚¼ê°í˜• ì–¼êµ´",
    "ìœ¡ê°í˜•": "ê°ì§„ ì–¼êµ´í˜•",
    "ë§ˆë¦„ëª¨í˜•": "ê¸´ì–¼êµ´í˜•",
    "ë•…ì½©í˜•": "ê°ì§„ ì–¼êµ´í˜•",
}


def build_auto_terms(app_face_shape: str, max_terms: int = 6) -> List[str]:
    if not app_face_shape:
        return ["ë ˆì´ì–´ë“œì»·", "Cì»¬íŒ", "Sì»¬íŒ"]
    reco_face = APP_FACE_TO_RECO_FACE.get(app_face_shape, "ê³„ë€í˜• ì–¼êµ´")
    terms = FACE_SHAPE_TO_KEYWORDS.get(reco_face, [])
    return terms[:max_terms] if terms else ["ë ˆì´ì–´ë“œì»·", "Cì»¬íŒ", "Sì»¬íŒ"]


# =========================
# GPT ì¶”ì²œ ìƒì„±(3ê°œ) - ì‹¤ì¡´ ìš©ì–´ë§Œ
# =========================
def safe_json_extract(text: str) -> str:
    raw = (text or "").strip()
    raw = re.sub(r"^```(?:json)?\s*", "", raw, flags=re.IGNORECASE)
    raw = re.sub(r"\s*```$", "", raw)
    m = re.search(r"\{.*\}", raw, flags=re.DOTALL)
    return m.group(0) if m else raw


def normalize_query(q: str) -> str:
    q = (q or "").replace("\n", " ").strip()
    q = re.sub(r"\s+", " ", q)
    if "ë¯¸ìš©ì‹¤" not in q:
        q = f"{q} ë¯¸ìš©ì‹¤".strip()
    return q


def enforce_style_whitelist(query: str, allowed_terms: List[str]) -> str:
    q = query.replace("ë¯¸ìš©ì‹¤", "").strip()
    terms_sorted = sorted(allowed_terms, key=len, reverse=True)
    picked: List[str] = []
    for t in terms_sorted:
        if t in q and t not in picked:
            picked.append(t)

    if not picked:
        picked = [allowed_terms[0]] if allowed_terms else ["ë ˆì´ì–´ë“œì»·"]

    picked = picked[:2]
    return normalize_query(" ".join(picked))


def make_queries_with_openai(
    *,
    api_key: str,
    tone: str,
    face_shape: str,
    brow_shape: str,
    hair_type: str,
    hint_terms: List[str],
) -> Tuple[List[str], List[str]]:
    client = OpenAI(api_key=api_key)
    allowed = STYLE_TERMS

    prompt = f"""
ë„ˆëŠ” í•œêµ­ í—¤ì–´ë””ìì´ë„ˆì•¼.
ì•„ë˜ ì‚¬ìš©ìì˜ ìê°€ì§„ë‹¨ ì •ë³´(í†¤/ì–¼êµ´í˜•/ëˆˆì¹/ëª¨ë°œ)ë¥¼ ë°”íƒ•ìœ¼ë¡œ
ì¹´ì¹´ì˜¤ ë¡œì»¬ì—ì„œ ê²€ìƒ‰ ê°€ëŠ¥í•œ "ë¯¸ìš©ì‹¤ ê²€ìƒ‰ í‚¤ì›Œë“œ 3ê°œ"ë¥¼ ì¶”ì²œí•´ì¤˜.

ì¤‘ìš” ê·œì¹™:
- ê° queryì—ëŠ” ë°˜ë“œì‹œ 'ë¯¸ìš©ì‹¤' í¬í•¨
- queryëŠ” ë°˜ë“œì‹œ "í—ˆìš©ëœ ìŠ¤íƒ€ì¼ ìš©ì–´ ëª©ë¡"ì—ì„œë§Œ ê³¨ë¼ ì¡°í•©
- í—ˆìš© ëª©ë¡ ë°– ë‹¨ì–´ ì ˆëŒ€ ê¸ˆì§€
- (ìŠ¤íƒ€ì¼ìš©ì–´ 1~2ê°œ + 'ë¯¸ìš©ì‹¤')ë¡œ ê°„ê²°í•˜ê²Œ

[ì‚¬ìš©ì ì •ë³´]
- tone: {tone}
- face_shape: {face_shape}
- brow_shape: {brow_shape}
- hair_type: {hair_type}

[ì¶”ì²œ íŒíŠ¸(ìš°ì„  ê³ ë ¤ ê°€ëŠ¥)]
{json.dumps(hint_terms, ensure_ascii=False)}

[í—ˆìš©ëœ ìŠ¤íƒ€ì¼ ìš©ì–´ ëª©ë¡]
{json.dumps(allowed, ensure_ascii=False)}

ì¶œë ¥(JSONë§Œ):
{{
  "recommendations": [
    {{"query":"... ë¯¸ìš©ì‹¤","reason":"..."}},
    {{"query":"... ë¯¸ìš©ì‹¤","reason":"..."}},
    {{"query":"... ë¯¸ìš©ì‹¤","reason":"..."}}
  ]
}}
""".strip()

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
    )

    raw = safe_json_extract(resp.choices[0].message.content or "")
    queries: List[str] = []
    reasons: List[str] = []

    try:
        obj = json.loads(raw)
        recs = obj.get("recommendations", [])
        if isinstance(recs, list):
            for it in recs:
                if not isinstance(it, dict):
                    continue
                queries.append(str(it.get("query", "")).strip())
                reasons.append(str(it.get("reason", "")).strip())
    except Exception:
        queries, reasons = [], []

    final_q: List[str] = []
    final_r: List[str] = []
    seen = set()

    for q, r in zip(queries, reasons):
        fixed = enforce_style_whitelist(q, allowed_terms=allowed)
        if fixed and fixed not in seen:
            seen.add(fixed)
            final_q.append(fixed)
            final_r.append(r)
        if len(final_q) >= 3:
            break

    if len(final_q) < 3:
        fallback_pool = []
        for t in hint_terms:
            if t in STYLE_TERMS:
                fallback_pool.append(normalize_query(f"{t} ë¯¸ìš©ì‹¤"))
        for t in STYLE_TERMS:
            fallback_pool.append(normalize_query(f"{t} ë¯¸ìš©ì‹¤"))

        for q in fallback_pool:
            if q not in seen:
                seen.add(q)
                final_q.append(q)
                final_r.append("í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ ë³´ì™„ ì¶”ì²œ")
            if len(final_q) >= 3:
                break

    return final_q[:3], final_r[:3]


# =========================
# Kakao Local + Kakao Search ìœ í‹¸
# =========================
def kakao_headers() -> Dict[str, str]:
    return {"Authorization": f"KakaoAK {KAKAO_REST_API_KEY}"}


@st.cache_data(show_spinner=False, ttl=3600)
def kakao_address_to_xy(address: str) -> Tuple[float, float]:
    url = "https://dapi.kakao.com/v2/local/search/address.json"
    r = requests.get(url, headers=kakao_headers(), params={"query": address}, timeout=10)
    r.raise_for_status()
    docs = r.json().get("documents", [])
    if not docs:
        raise ValueError("ì£¼ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. ë” ìì„¸í•œ ì£¼ì†Œë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    return float(docs[0]["x"]), float(docs[0]["y"])


@st.cache_data(show_spinner=False, ttl=600)
def kakao_keyword_search(
    query: str,
    x: float,
    y: float,
    radius_m: int = 3000,
    size: int = 15,
    page: int = 1,
) -> List[dict]:
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    params = {
        "query": query,
        "x": str(x),
        "y": str(y),
        "radius": str(radius_m),
        "size": str(size),
        "page": str(page),
        "sort": "distance",
    }
    r = requests.get(url, headers=kakao_headers(), params=params, timeout=10)
    r.raise_for_status()
    return r.json().get("documents", [])


def strip_html(text: str) -> str:
    text = re.sub(r"<[^>]+>", " ", text)
    return re.sub(r"\s+", " ", text).strip()


@st.cache_data(show_spinner=False, ttl=1800)
def kakao_search_blog(query: str, size: int = 5) -> List[dict]:
    url = "https://dapi.kakao.com/v2/search/blog"
    params = {"query": query, "size": size, "sort": "accuracy"}
    r = requests.get(url, headers=kakao_headers(), params=params, timeout=10)
    r.raise_for_status()
    return r.json().get("documents", [])


@st.cache_data(show_spinner=False, ttl=1800)
def kakao_search_web(query: str, size: int = 5) -> List[dict]:
    url = "https://dapi.kakao.com/v2/search/web"
    params = {"query": query, "size": size, "sort": "accuracy"}
    r = requests.get(url, headers=kakao_headers(), params=params, timeout=10)
    r.raise_for_status()
    return r.json().get("documents", [])


def build_review_snippet_for_place(place_name: str, area_hint: str) -> str:
    q = f"{place_name} {area_hint} ë¯¸ìš©ì‹¤ í›„ê¸° íŒ ì»·"
    blog_docs = []
    web_docs = []
    try:
        blog_docs = kakao_search_blog(q, size=5)
    except Exception:
        blog_docs = []
    try:
        web_docs = kakao_search_web(q, size=5)
    except Exception:
        web_docs = []

    parts: List[str] = []
    for d in blog_docs[:5]:
        parts.append(f"[ë¸”ë¡œê·¸] {strip_html(d.get('title',''))} - {strip_html(d.get('contents',''))}")
    for d in web_docs[:5]:
        parts.append(f"[ì›¹] {strip_html(d.get('title',''))} - {strip_html(d.get('contents',''))}")

    return " | ".join([p for p in parts if p.strip()])[:2500]


def analyze_styles_from_reviews_with_openai(
    *,
    api_key: str,
    chosen_query: str,
    places: List[dict],
    review_snippets: Dict[str, str],
) -> Dict[str, Dict]:
    client = OpenAI(api_key=api_key)

    payload = []
    for p in places:
        name = p.get("place_name", "")
        addr = p.get("road_address_name", "") or p.get("address_name", "")
        snip = review_snippets.get(name, "")
        payload.append({"name": name, "address": addr, "snippet": snip})

    prompt = f"""
ë„ˆëŠ” í•œêµ­ í—¤ì–´/ë¯¸ìš©ì‹¤ ë¦¬ë·° ë¶„ì„ê°€ì•¼.
ì‚¬ìš©ìì˜ ì˜ë„ í‚¤ì›Œë“œì™€ ê° ë¯¸ìš©ì‹¤ì˜ ì›¹ í›„ê¸° ìŠ¤ë‹ˆí«ì„ ë³´ê³ ,
ê° ë¯¸ìš©ì‹¤ì´ ìœ ëª…í•œ ì‹œìˆ /ìŠ¤íƒ€ì¼ íƒœê·¸ë¥¼ ë½‘ì•„ì¤˜.

ê·œì¹™:
- tagsëŠ” ë°˜ë“œì‹œ ì•„ë˜ í—ˆìš© ëª©ë¡ì—ì„œë§Œ ì„ íƒ
- snippetì´ ë¹ˆ ê²½ìš° tags=[], summary="ì •ë³´ ë¶€ì¡±"
- JSONë§Œ ì¶œë ¥

[chosen_query] {chosen_query}
[í—ˆìš©ëœ ìŠ¤íƒ€ì¼ ìš©ì–´ ëª©ë¡] {json.dumps(STYLE_TERMS, ensure_ascii=False)}
[ë°ì´í„°] {json.dumps(payload, ensure_ascii=False)}

ì¶œë ¥:
{{"salons":[{{"name":"...","tags":["..."],"summary":"..."}}, ...]}}
""".strip()

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
    )

    raw = safe_json_extract(resp.choices[0].message.content or "")
    result: Dict[str, Dict] = {}
    try:
        obj = json.loads(raw)
        salons = obj.get("salons", [])
        if isinstance(salons, list):
            for s in salons:
                if not isinstance(s, dict):
                    continue
                name = str(s.get("name", "")).strip()
                tags = s.get("tags", [])
                summary = str(s.get("summary", "")).strip()
                if isinstance(tags, list):
                    tags = [str(t).strip() for t in tags if str(t).strip() in STYLE_TERMS]
                else:
                    tags = []
                if name:
                    result[name] = {"tags": tags[:6], "summary": summary}
    except Exception:
        result = {}
    return result


def build_expanded_queries_from_tags(
    chosen_query: str,
    style_map: Dict[str, Dict],
    max_queries: int = 3,
) -> List[str]:
    counter = Counter()
    for v in style_map.values():
        for t in v.get("tags", []):
            if t and t not in STYLE_STOP:
                counter[t] += 1

    ranked = [t for t, _ in counter.most_common()]

    chosen_words = set(re.findall(r"[ê°€-í£A-Za-z0-9]+", chosen_query))
    ranked = [t for t in ranked if t not in chosen_words]

    expanded = [normalize_query(f"{t} ë¯¸ìš©ì‹¤") for t in ranked[:max_queries]]
    # ì¤‘ë³µ ì œê±°
    uniq, seen = [], set()
    for q in expanded:
        if q not in seen:
            seen.add(q)
            uniq.append(q)
    return uniq[:max_queries]


def merge_places(*lists: List[dict]) -> List[dict]:
    merged = []
    seen = set()
    for lst in lists:
        for p in lst:
            key = p.get("place_url") or (p.get("place_name", "") + "|" + (p.get("road_address_name", "") or p.get("address_name", "")))
            if key and key not in seen:
                seen.add(key)
                merged.append(p)
    return merged


# =========================
# âœ… ê²€ìƒ‰ ê²°ê³¼ê°€ ì˜ ë‚˜ì˜¤ê²Œ í•˜ëŠ” í•µì‹¬: fallback ê²€ìƒ‰
# =========================
def build_fallback_queries(chosen_query: str) -> List[str]:
    q = (chosen_query or "").strip()
    q_no_salon = q.replace("ë¯¸ìš©ì‹¤", "").strip()
    fallbacks = []

    if q:
        fallbacks.append(q)
    if q_no_salon:
        fallbacks.append(q_no_salon)
        fallbacks.append(f"{q_no_salon} í—¤ì–´")
        fallbacks.append(f"{q_no_salon} í—¤ì–´ìƒµ")

    # ìµœí›„ì˜ ë³´ë£¨ (ê±°ì˜ ë¬´ì¡°ê±´ ê²°ê³¼ ìˆìŒ)
    fallbacks.append("ë¯¸ìš©ì‹¤")
    # ì¤‘ë³µ ì œê±°
    uniq = []
    seen = set()
    for x in fallbacks:
        x = re.sub(r"\s+", " ", x).strip()
        if x and x not in seen:
            seen.add(x)
            uniq.append(x)
    return uniq


def search_salons_with_fallback(
    *,
    chosen_query: str,
    x: float,
    y: float,
    radius_m: int,
    size: int = 15,
) -> Tuple[List[dict], str, int]:
    """
    (results, used_query, used_radius)
    - queryë¥¼ ì™„í™”í•˜ë©° ì‹œë„
    - ê²°ê³¼ê°€ ì—†ìœ¼ë©´ radiusë¥¼ 2ë°°ê¹Œì§€ ëŠ˜ë ¤ì„œ ì¬ì‹œë„
    """
    queries = build_fallback_queries(chosen_query)
    radius_try = [radius_m, min(radius_m * 2, 20000)]

    for r in radius_try:
        for q in queries:
            # page=1ë§Œìœ¼ë¡œ ë¶€ì¡±í•˜ë©´ 2í˜ì´ì§€ê¹Œì§€ ì¶”ê°€(ê²°ê³¼ ëŠ˜ë¦¬ê¸°)
            res1 = kakao_keyword_search(query=q, x=x, y=y, radius_m=r, size=size, page=1)
            res2 = kakao_keyword_search(query=q, x=x, y=y, radius_m=r, size=size, page=2) if res1 else []
            res = merge_places(res1, res2)
            if res:
                return res, q, r

    return [], queries[0] if queries else chosen_query, radius_m


# =========================
# 1) ì„ íƒ UI
# =========================
for k in ("tone", "face_shape", "brow_shape", "hair_type"):
    if k not in st.session_state:
        st.session_state[k] = None

steps_done = sum(1 for k in ("tone", "face_shape", "brow_shape", "hair_type") if st.session_state[k] is not None)
st.progress(steps_done / 4)

# ---- tone
st.header("1) ì›œí†¤ / ì¿¨í†¤ ì„ íƒ")
tone_cols = st.columns(2, gap="large")
with tone_cols[0]:
    select_card(
        title="ì›œí†¤",
        image_path="ì›œí†¤.jpg",
        desc_md="**ìê°€ì§„ë‹¨**\n1. íŒ”ëª© í˜ˆê´€ì´ **ì´ˆë¡ë¹›**\n2. í”¼ë¶€ì— **ë…¸ë€ê¸°**ê°€ ë§ìŒ",
        button_label="âœ… ì›œí†¤ ì„ íƒ",
        on_click_value="ì›œ",
        session_key="tone",
        button_key="btn_tone_warm",
        img_width=100,
        selected=(st.session_state["tone"] == "ì›œ"),
    )
with tone_cols[1]:
    select_card(
        title="ì¿¨í†¤",
        image_path="ì¿¨í†¤.jpg",
        desc_md="**ìê°€ì§„ë‹¨**\n1. íŒ”ëª© í˜ˆê´€ì´ **íŒŒë€ë¹›**\n2. í”¼ë¶€ì— **ë¶‰ì€ê¸°**ê°€ ë§ìŒ",
        button_label="âœ… ì¿¨í†¤ ì„ íƒ",
        on_click_value="ì¿¨",
        session_key="tone",
        button_key="btn_tone_cool",
        img_width=100,
        selected=(st.session_state["tone"] == "ì¿¨"),
    )

if st.button("tone ì´ˆê¸°í™”", key="reset_tone", type="secondary"):
    st.session_state["tone"] = None
    st.rerun()

st.divider()

# ---- face
st.header("2) ì–¼êµ´í˜• ì„ íƒ")
FACE_CHOICES = [
    ("ê³„ë€í˜•", "ê³„ë€í˜•.png", "ê´‘ëŒ€ X, í„± X - ê´‘ëŒ€ì™€ í„± ê³¨ê²©ì´ ì´ìƒì ìœ¼ë¡œ ì¡í˜€ìˆê³  ëˆˆì— ë„ê²Œ ëŒì¶œë˜ì–´ ìˆì§€ ì•ŠìŒ."),
    ("ë§ˆë¦„ëª¨í˜•", "ë§ˆë¦„ëª¨í˜•.png", "ê´‘ëŒ€ O, í„± X - ì˜†í„± ê³¨ê²©ì€ ì—†ëŠ”ë° ê´‘ëŒ€ë§Œ ë¶€ê°ë¨."),
    ("í•˜íŠ¸í˜•", "í•˜íŠ¸í˜•.png", "ê´‘ëŒ€ O í„± â–³ - ê´‘ëŒ€ ê³¨ê²©ê³¼ í„± ê³¨ê²©ì´ ëª¨ë‘ ìˆëŠ”ë° ê´‘ëŒ€ ê³¨ê²©ì´ í„± ê³¨ê²©ë³´ë‹¤ ë„“ê³  ê°•í•¨."),
    ("ë•…ì½©í˜•", "ë•…ì½©í˜•.png", "ê´‘ëŒ€ O í„± O - ê´‘ëŒ€ ê³¨ê²©ê³¼ í„± ê³¨ê²©ì´ ëª¨ë‘ ìˆê³ , í•˜íŠ¸í˜•ê³¼ ë‹¤ë¥´ê²Œ ê´‘ëŒ€ ê³¨ê²©ê³¼ í„± ê³¨ê²©ì˜ ë„ˆë¹„ê°€ ê°™ìŒ."),
    ("ìœ¡ê°í˜•", "ìœ¡ê°í˜•.png", "ê´‘ëŒ€ X í„± O - í„± ê³¨ê²©ë§Œ ìˆê³  ë•…ì½©í˜•ê³¼ëŠ” ë‹¤ë¥´ê²Œ ì˜†ìœ¼ë¡œ íŠ€ì–´ë‚˜ì˜¨ ê´‘ëŒ€ ê³¨ê²©ì´ ì—†ìŒ."),
    ("ë‘¥ê·¼í˜•", "ë‘¥ê·¼í˜•.png", "ê´‘ëŒ€ X í„± X - ì–¼êµ´ ì „ì²´ì ìœ¼ë¡œ ì‚´ì´ ë§ì•„ í…Œíˆ¬ë¦¬ì— ê³¨ê²©ì´ ì˜ ë³´ì´ì§€ ì•ŠìŒ."),
]
rows = [FACE_CHOICES[:3], FACE_CHOICES[3:]]
for r_i, r in enumerate(rows):
    cols = st.columns(3, gap="large")
    for col, (name, img, desc) in zip(cols, r):
        with col:
            select_card(
                title=name,
                image_path=img,
                desc_md=desc,
                button_label=f"âœ… {name} ì„ íƒ",
                on_click_value=name,
                session_key="face_shape",
                button_key=f"btn_face_{r_i}_{name}",
                img_width=160,
                selected=(st.session_state["face_shape"] == name),
            )

if st.button("face_shape ì´ˆê¸°í™”", key="reset_face", type="secondary"):
    st.session_state["face_shape"] = None
    st.rerun()

st.divider()

# ---- brow
st.header("3) ëˆˆì¹ ëª¨ì–‘ ì„ íƒ")
BROW_CHOICES = [("ì•„ì¹˜í˜•", "ì•„ì¹˜í˜•.png"), ("ì§ì„ í˜•", "ì§ì„ í˜•.png"), ("ê°ì§„í˜•", "ê°ì§„í˜•.png"), ("ë‘¥ê·¼í˜•", "ë‘¥ê·¼í˜•(ëˆˆì¹).png")]
brow_rows = [BROW_CHOICES[:2], BROW_CHOICES[2:]]
for r_i, r in enumerate(brow_rows):
    cols = st.columns(2, gap="large")
    for col, (name, img) in zip(cols, r):
        with col:
            select_card(
                title=name,
                image_path=img,
                button_label=f"âœ… {name} ì„ íƒ",
                on_click_value=name,
                session_key="brow_shape",
                button_key=f"btn_brow_{r_i}_{name}",
                img_width=100,
                selected=(st.session_state["brow_shape"] == name),
            )

if st.button("brow_shape ì´ˆê¸°í™”", key="reset_brow", type="secondary"):
    st.session_state["brow_shape"] = None
    st.rerun()

st.divider()

# ---- hair type
st.header("4) ëª¨ë°œ íƒ€ì… ì„ íƒ")
hair_cols = st.columns(2, gap="large")
with hair_cols[0]:
    select_card(
        title="ì§ëª¨",
        image_path="ì§ëª¨.png",
        button_label="âœ… ì§ëª¨ ì„ íƒ",
        on_click_value="ì§ëª¨",
        session_key="hair_type",
        button_key="btn_hair_straight",
        img_width=80,
        selected=(st.session_state["hair_type"] == "ì§ëª¨"),
    )
with hair_cols[1]:
    select_card(
        title="ê³±ìŠ¬",
        image_path="ê³±ìŠ¬.png",
        button_label="âœ… ê³±ìŠ¬ ì„ íƒ",
        on_click_value="ê³±ìŠ¬",
        session_key="hair_type",
        button_key="btn_hair_curly",
        img_width=80,
        selected=(st.session_state["hair_type"] == "ê³±ìŠ¬"),
    )

if st.button("hair_type ì´ˆê¸°í™”", key="reset_hair", type="secondary"):
    st.session_state["hair_type"] = None
    st.rerun()

st.divider()


# =========================
# 5) GPT ì¶”ì²œ(3ê°œ)
# =========================
st.header("5) GPT ì¶”ì²œ í‚¤ì›Œë“œ 3ê°œ(ì‹¤ì¡´ ìŠ¤íƒ€ì¼ ìš©ì–´)")

tone = st.session_state["tone"]
face_shape = st.session_state["face_shape"]
brow_shape = st.session_state["brow_shape"]
hair_type = st.session_state["hair_type"]

m1, m2, m3, m4 = st.columns(4)
m1.metric("tone", tone or "-")
m2.metric("face_shape", face_shape or "-")
m3.metric("brow_shape", brow_shape or "-")
m4.metric("hair_type", hair_type or "-")

all_selected = all([tone, face_shape, brow_shape, hair_type])
has_openai_key = bool(OPENAI_API_KEY)

hint_terms = build_auto_terms(face_shape)

if "gpt_queries" not in st.session_state:
    st.session_state["gpt_queries"] = []
if "gpt_reasons" not in st.session_state:
    st.session_state["gpt_reasons"] = []

if not has_openai_key:
    st.warning("OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤. (ì‚¬ì´ë“œë°”ì—ì„œ ì…ë ¥)")
elif not all_selected:
    st.warning("tone/face_shape/brow_shape/hair_typeë¥¼ ëª¨ë‘ ì„ íƒí•˜ë©´ GPT ì¶”ì²œ(3ê°œ)ìœ¼ë¡œë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
else:
    st.success("âœ… ëª¨ë“  ì„ íƒ ì™„ë£Œ â†’ GPT ì¶”ì²œ(3ê°œ)ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

gpt_btn = st.button(
    "âœ¨ GPT ì¶”ì²œ ê²€ìƒ‰ì–´ 3ê°œ ë§Œë“¤ê¸°",
    key="btn_make_gpt_queries",
    use_container_width=True,
    disabled=(not has_openai_key or not all_selected),
)

if gpt_btn:
    try:
        with st.spinner("GPTê°€ ê²€ìƒ‰ì–´ 3ê°œë¥¼ ì¶”ì²œí•˜ëŠ” ì¤‘..."):
            qs, rs = make_queries_with_openai(
                api_key=OPENAI_API_KEY,
                tone=tone,
                face_shape=face_shape,
                brow_shape=brow_shape,
                hair_type=hair_type,
                hint_terms=hint_terms,
            )
            st.session_state["gpt_queries"] = qs
            st.session_state["gpt_reasons"] = rs
    except Exception as e:
        st.error(f"GPT í˜¸ì¶œ ì˜¤ë¥˜: {e}")

chosen_query = ""
chosen_idx = 0
if all_selected and has_openai_key and st.session_state["gpt_queries"]:
    options = [f"ğŸ¤– GPT ì¶”ì²œ {i+1}: {q}" for i, q in enumerate(st.session_state["gpt_queries"])]
    chosen = st.radio(
        "ì•„ë˜ GPT ì¶”ì²œ í‚¤ì›Œë“œ(3ê°œ) ì¤‘ í•˜ë‚˜ë¡œ 1ì°¨ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
        options=options,
        index=0,
        key="auto_query_radio",
    )
    chosen_idx = options.index(chosen)
    chosen_query = st.session_state["gpt_queries"][chosen_idx]
    reason = st.session_state["gpt_reasons"][chosen_idx] if chosen_idx < len(st.session_state["gpt_reasons"]) else ""
    if reason:
        st.info(f"GPT ì¶”ì²œ ì´ìœ : {reason}")

st.divider()


# =========================
# 6) Kakao Local ê²€ìƒ‰ + ì›¹í›„ê¸° ê¸°ë°˜ í™•ì¥ê²€ìƒ‰
# =========================
st.header("6) (ì›¹ í›„ê¸° ë¶„ì„)ìœ¼ë¡œ ìœ ëª… ìŠ¤íƒ€ì¼ì„ ì°¾ê³  í™•ì¥ ê²€ìƒ‰í•˜ê¸°")

address = st.text_input("ë‚´ ìœ„ì¹˜(ì£¼ì†Œ)ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” (ì˜ˆ: ì„œìš¸ì‹œ ì„œëŒ€ë¬¸êµ¬ ì—°ì„¸ë¡œ 50)", key="input_address")
radius = st.slider("ê²€ìƒ‰ ë°˜ê²½(ë¯¸í„°)", 500, 10000, 3000, step=500, key="radius_slider")

use_review_expansion = st.toggle("ì›¹ í›„ê¸° ê¸°ë°˜ í™•ì¥ê²€ìƒ‰ ì‚¬ìš©", value=True)
topn_for_review = st.slider("í›„ê¸° ë¶„ì„í•  í›„ë³´ ê°œìˆ˜(ìƒìœ„ Nê°œ)", 3, 15, 10, step=1)
expansion_queries_n = st.slider("í™•ì¥ ê²€ìƒ‰ì–´ ê°œìˆ˜", 1, 3, 3, step=1)

# âœ… ì¶”ê°€: â€œê²€ìƒ‰ ê²°ê³¼ ì˜ ë‚˜ì˜¤ê²Œâ€ ì˜µì…˜
st.subheader("ê²€ìƒ‰ ê²°ê³¼ê°€ ì˜ ë‚˜ì˜¤ê²Œ í•˜ëŠ” ì˜µì…˜")
auto_fallback = st.toggle("ê²€ìƒ‰ì–´ ìë™ ì™„í™”(fallback) ì‚¬ìš©", value=True)
auto_increase_radius = st.toggle("ê²°ê³¼ ì—†ìœ¼ë©´ ë°˜ê²½ ìë™ í™•ëŒ€(ìµœëŒ€ 2ë°°)", value=True)
result_size = st.slider("ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜(size)", 5, 20, 15, step=1)

find_btn = st.button("ğŸ“ (1ì°¨+í™•ì¥) ê·¼ì²˜ ë¯¸ìš©ì‹¤ ì°¾ê¸°", key="btn_find_salon", use_container_width=True)

if find_btn:
    if not has_openai_key:
        st.error("OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤. (ì‚¬ì´ë“œë°”ì—ì„œ ì…ë ¥)")
        st.stop()
    if not all_selected:
        st.error("tone/face_shape/brow_shape/hair_typeë¥¼ ëª¨ë‘ ì„ íƒí•´ì£¼ì„¸ìš”.")
        st.stop()
    if not st.session_state["gpt_queries"] or not chosen_query.strip():
        st.error("ë¨¼ì € GPT ì¶”ì²œ ê²€ìƒ‰ì–´(3ê°œ)ë¥¼ ìƒì„±í•˜ê³  í•˜ë‚˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        st.stop()
    if not address.strip():
        st.error("ì£¼ì†Œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

    try:
        with st.spinner("ì£¼ì†Œë¥¼ ì¢Œí‘œë¡œ ë³€í™˜ ì¤‘..."):
            x, y = kakao_address_to_xy(address.strip())

        # 1ì°¨ ê²€ìƒ‰ (ìë™ ì™„í™” ì ìš©)
        with st.spinner("1ì°¨: ë¯¸ìš©ì‹¤ ê²€ìƒ‰ ì¤‘..."):
            if auto_fallback:
                base_results, used_q, used_r = search_salons_with_fallback(
                    chosen_query=chosen_query,
                    x=x, y=y,
                    radius_m=radius,
                    size=result_size,
                )
                if (not auto_increase_radius) and used_r != radius:
                    # ì‚¬ìš©ìê°€ ìë™ í™•ëŒ€ OFFë©´, í™•ëŒ€ëœ ê²°ê³¼ëŠ” ì“°ì§€ ì•Šë„ë¡ ì²˜ë¦¬
                    if used_r != radius:
                        # ë‹¤ì‹œ radius ê³ ì •ìœ¼ë¡œë§Œ ì‹œë„
                        base_results, used_q, used_r = search_salons_with_fallback(
                            chosen_query=chosen_query,
                            x=x, y=y,
                            radius_m=radius,
                            size=result_size,
                        )
            else:
                base_results = kakao_keyword_search(query=chosen_query, x=x, y=y, radius_m=radius, size=result_size, page=1)
                used_q, used_r = chosen_query, radius

        if not base_results:
            st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ìš”. (ìë™ ì™„í™”/ë°˜ê²½ í™•ëŒ€ ì˜µì…˜ì„ ì¼œë³´ì„¸ìš”.)")
            st.stop()

        st.success(f"1ì°¨ ê²€ìƒ‰ ì„±ê³µ: '{used_q}' / ë°˜ê²½ {used_r}m / ê²°ê³¼ {len(base_results)}ê°œ")

        merged_results = base_results
        style_map: Dict[str, Dict] = {}
        expanded_queries: List[str] = []

        # ì›¹í›„ê¸° ê¸°ë°˜ í™•ì¥ê²€ìƒ‰
        if use_review_expansion and base_results:
            candidates = base_results[:topn_for_review]
            area_hint = " ".join(address.strip().split()[:3]) or address.strip()

            with st.spinner("ì›¹ í›„ê¸°(ë¸”ë¡œê·¸/ì›¹ë¬¸ì„œ) ìŠ¤ë‹ˆí« ìˆ˜ì§‘ ì¤‘..."):
                snippets: Dict[str, str] = {}
                for p in candidates:
                    name = p.get("place_name", "")
                    if not name:
                        continue
                    snippets[name] = build_review_snippet_for_place(name, area_hint)

            with st.spinner("ì›¹ í›„ê¸° ê¸°ë°˜ ìœ ëª… ìŠ¤íƒ€ì¼ íƒœê·¸ ë¶„ì„(GPT)..."):
                style_map = analyze_styles_from_reviews_with_openai(
                    api_key=OPENAI_API_KEY,
                    chosen_query=chosen_query,
                    places=candidates,
                    review_snippets=snippets,
                )

            expanded_queries = build_expanded_queries_from_tags(
                chosen_query=chosen_query,
                style_map=style_map,
                max_queries=expansion_queries_n,
            )

            extra_lists: List[List[dict]] = []
            if expanded_queries:
                with st.spinner("í™•ì¥ ê²€ìƒ‰ì–´ë¡œ ì¶”ê°€ ê²€ìƒ‰ ì¤‘..."):
                    for q in expanded_queries:
                        if auto_fallback:
                            res, _, _ = search_salons_with_fallback(
                                chosen_query=q, x=x, y=y, radius_m=radius, size=result_size
                            )
                            extra_lists.append(res)
                        else:
                            extra_lists.append(kakao_keyword_search(query=q, x=x, y=y, radius_m=radius, size=result_size, page=1))
                merged_results = merge_places(base_results, *extra_lists)

        # ì¶œë ¥
        st.success(f"ìµœì¢… ê²°ê³¼ {len(merged_results)}ê°œ")
        if expanded_queries:
            st.write("í™•ì¥ ê²€ìƒ‰ì–´(í›„ê¸° ê¸°ë°˜):", ", ".join(expanded_queries))

        map_points = [
            {"lat": float(r["y"]), "lon": float(r["x"])}
            for r in merged_results
            if r.get("x") and r.get("y")
        ]
        if map_points:
            st.map(map_points, zoom=13)

        st.subheader("ë¯¸ìš©ì‹¤ ë¦¬ìŠ¤íŠ¸")
        for i, r in enumerate(merged_results, start=1):
            name = r.get("place_name", "")
            road = r.get("road_address_name", "") or r.get("address_name", "")
            phone = r.get("phone", "")
            dist = r.get("distance", "")
            url = r.get("place_url", "")

            st.markdown(f"### {i}. {name}")
            if dist:
                st.write(f"- ê±°ë¦¬: **{dist}m**")
            st.write(f"- ì£¼ì†Œ: {road}")
            if phone:
                st.write(f"- ì „í™”: {phone}")
            if url:
                st.write(f"- ì¹´ì¹´ì˜¤ë§µ: {url}")

            if use_review_expansion and name in style_map:
                tags = style_map[name].get("tags", [])
                summary = style_map[name].get("summary", "")
                if tags:
                    st.write("- ì›¹ í›„ê¸° ê¸°ë°˜ ìœ ëª… ìŠ¤íƒ€ì¼:", " / ".join(tags))
                if summary:
                    st.caption(f"í›„ê¸° ìš”ì•½: {summary}")

    except Exception as e:
        st.error(f"ì˜¤ë¥˜: {e}")

st.divider()

if st.button("ì „ì²´ ì„ íƒ/ê²°ê³¼ ì´ˆê¸°í™”", key="reset_all", type="secondary"):
    for k in ("tone", "face_shape", "brow_shape", "hair_type"):
        st.session_state[k] = None
    st.session_state["gpt_queries"] = []
    st.session_state["gpt_reasons"] = []
    st.rerun()












